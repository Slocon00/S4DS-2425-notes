\section{Hypothesis Testing}

The statistical methods explored until now are used to infer knowledge about features of the model distribution that represent quantities of interest, in the form of numerical estimates. Sometimes, the conclusion to be drawn is not numerical, but rather a decision about the validity of a claim.  The process of formulating the possible conclusions of an experiment and choosing between two alternatives is known as hypothesis testing.

The two alternatives are called the \textbf{null hypothesis} $H_0$ and the \textbf{alternative hypothesis} $H_1$. The null hypothesis is presumed to be true until evidence through testing leads to its rejection in favor of the alternative one. The alternative hypothesis is a claim that goes against the null hypothesis. Since the null hypothesis is assumed to be true, we cannot accept it, and instead we say that we cannot reject it. To decide whether to reject the null hypothesis, we need to define a test statistic.
\boxdefinition{Test statistic}{
    Suppose the dataset is modeled as the realization of random variables $X_1, \ldots, X_n$. A test statistic is any sample statistic $T = h(X_1, \ldots, X_n)$ whose numerical value is used to decide whether we reject $H_0$.
}
Once we define the test statistic, we can calculate its value (``t-value'') under the null hypothesis and calculate the probability (``p-value'') of observing that value: if it is very unlikely, we reject the null hypothesis.

\paragraph{One-tailed test}
\begin{align*}
    &H_0 : \theta = v &H_1 : \theta < v (\text{or } H_1 : \theta > v)
\end{align*}
\begin{align*}
    &(1-\alpha),\ \alpha &\text{Confidence level, significance level}\\
    &T = h(X_1, \ldots, X_n) &\text{Test statistic}\\
    &t = h(x_1, \ldots, x_n) &\text{t-value}\\
    &p = P(T \leq t) \quad (\text{or } P(T \geq t)) &\text{p-value}\\
    &c_l \text{ s.t. } P(T \leq c_l) = \alpha \quad (\text{or } c_u \text{ s.t. } P(T \geq c_u) = \alpha) &\text{Critical value} \\
    &[-\infty, c_l] \quad (\text{or } [c_u, \infty]) &\text{Critical region}
\end{align*}
If $t \leq c_l$ (or $t \geq c_u$) $\implies$ $H_0$ is rejected. \\
If $p \leq \alpha$ $\implies$ $H_0$ is rejected. \\

\paragraph{Two-tailed test}
\begin{align*}
    &H_0 : \theta = v &H_1 : \theta \not = v
\end{align*}
\begin{align*}
    &p = P(T \leq t) + P(T \geq t) &\text{p-value}\\
    &c_l \text{ s.t. } P(T \leq c_l) = \frac{\alpha}{2} \text{ and } c_u \text{ s.t. } P(T \geq c_u) = \frac{\alpha}{2} &\text{Critical values} \\
    &[-\infty, c_l] \land [c_u, \infty] &\text{Critical region}
\end{align*}
Hypothesis testing can be related to confidence intervals. Suppose we have a test for the mean in the form:
\begin{align*}
    H_0 : \mu = v
    H_1 : \mu > v
\end{align*}
where $\alpha = 0.05$, and the test statistic $T$ is a mean of $n$ random variables, so it has a normal distribution $\mathcal{N}(\mu, \sigma)$. The null hypothesis is rejected when:
\begin{gather*}
    t = \bar{x}_n \geq c_u \\
    \iff \bar{x}_n \geq v + z_{0.05} \cdot \sigma \\
    \iff v \leq \bar{x}_n - z_{0.05} \cdot \sigma \\
    \iff v \not \in \text{95\% one tailed C.I. for } \mu
\end{gather*}
In other words, the interval $(\bar{x}_n - z_{0.05} \cdot \sigma, \infty)$ is a one-tailed confidence interval for the mean $\mu$.

\subsection{Type I and Type II errors}
Errors in hypothesis testing can be of two types:
\begin{itemize}
    \item \textbf{Type I error} (\textbf{$\alpha$-risk}, \textbf{false positive rate}): we falsely reject the null hypothesis when it is true. The probability of this error occurring is equal to the significance level $\alpha$, since we reject $H_0$ when $p < \alpha$. This error can be controlled by choosing the largest appropriate significance level. An alternative is to simply report the p-value, and let the decision makers to choose their own level.
    \item \textbf{Type II error} (\textbf{$\beta$-risk}, \textbf{false negative rate}): we falsely do not reject the null hypothesis when the alternative one is true. The probability of this error occurring is called \textbf{power} of the test, and is $1 - \beta$.
\end{itemize}
Type II error can be arbitrarily close to the confidence level $1 - \alpha$: this is because the true distribution of the test statistic and the distribution under the null hypothesis can overlap significantly. Obviously the error probability never reaches $1-\alpha$, because that can only happen if the two distributions are identical, i.e., the null hypothesis is true.

\subsection{One sample tests for the mean}

Let $x_1, \ldots, x_n$ be the realizations of $X_1, \ldots, X_n \sim F$, where $\E[X_i] = \mu$ and $Var(X_i)=\sigma^2$. How consistent is the dataset with the (null) hypothesis that the mean is equal to a specific value ($\mu = \mu_0$)? We can distinguish three cases, depending on the assumed distribution of the random variables: normally distributed data with known or unknown variance, general data with unknown variance, and bernoulli data.

\subsubsection{Normal data}

Let $X_1, \ldots, X_n \sim \mathcal{N}(\mu, \sigma^2)$ be our random sample, and $x_1, \ldots, x_n$ the dataset. We set up a two-tailed test, choosing the hypotheses:
\begin{align*}
    &H_0 : \mu = \mu_0 &H_1 : \mu \not = \mu_0
\end{align*}
and fixing the significance level $\alpha$. Depending on whether we know the variance or not, we will build the test statistic accordingly.

\paragraph{Known variance: z-test}
The test statistic is:
\begin{equation*}
    Z = \frac{\bar{X}_n - \mu_0}{\frac{\sigma}{\sqrt{n}}} \sim \mathcal{N}(0,1)
\end{equation*}
Let $z$ be its realization. Since this is a two-tailed test, we find the critical values $-z_{\alpha/2}, z_{\alpha/2}$:
\begin{align*}
    &P(Z \leq -z_{\alpha/2}) = \frac{\alpha}{2} &P(Z \geq z_{\alpha/2}) = \frac{\alpha}{2}
\end{align*}
Finally, if $|z| \geq z_{\alpha/2}$ we reject the null hypothesis; otherwise, we cannot reject it.

\paragraph{Unknown variance: t-test}
The test statistic is:
\begin{equation*}
    T = \frac{\bar{X}_n - \mu_0}{\frac{S_n}{\sqrt{n}}} \sim t(n-1)
\end{equation*}
where $S_n$ is the sample standard deviation estimated from the dataset. Let $t$ be its realization. As above, we find the critical values $-t_{\alpha/2, n-1}, t_{\alpha/2, n-1}$:
\begin{align*}
    &P(T \leq -t_{\alpha/2, n-1}) = \frac{\alpha}{2} &P(T \geq t_{\alpha/2, n-1}) = \frac{\alpha}{2}
\end{align*}
If $|t| \geq t_{\alpha/2, n-1}$ we reject the null hypothesis; otherwise, we cannot reject it.

\subsubsection{General data}
\paragraph{Large sample size: z-test or t-test}
If the sample size $n$ is large enough, then, for a variant of the CLT, the distribution of the test statistic
\begin{equation*}
    T = \frac{\bar{X}_n - \mu_0}{\frac{S_n}{\sqrt{n}}}
\end{equation*} 
is standard normal. We can then either use the z-test and approximate $\sigma^2$ with $s^2$, or we exploit the fact that $t(x) \to \mathcal{N}(0,1)$ for $n \to \infty$ and use the t-test directly.

\paragraph{Symmetric distribution: Wilcoxon signed-rank test}
Let the random sample be $X_1, \ldots, X_n \sim F$, where $f(\mu-x) = f(\mu+x)$, i.e., the distribution is symmetric around the mean. The test statistic is:
\begin{equation*}
    W = \min \left\{ \sum \textit{rank}^+, \sum \textit{rank}^- \right\}
\end{equation*}
where $\textit{rank}^+$ are the ranks of the instances which are greater than $\mu_0$, and $\textit{rank}^-$ are the ranks of the instances which are less than $\mu_0$; ranks are assigned with respect to the absolute value of the differences with the mean according to the null hypothesis, $|x_i - \mu_0|$. Cases where the difference is 0 are ignored, and ties are handled by looking at the mean value of the instances involved in the tie.

If $n > 50$, the distribution of $W$ is approximately normal. If $n < 50$, we use the exact distribution of $W$, which is the ``null distribution''. As long as the distribution is symmetric, the same test is valid for the median of the distribution.

\paragraph{Bootstrap t-test}
Let $x_1, \ldots, x_n$ be the dataset. We estimate the empirical distribution of the random sample variables, use it to generate a number of bootstrap samples, and compute the studentized mean for each dataset:
\begin{equation*}
    t^* = \frac{\bar{x}_n^* - \bar{x}_n}{s_n^* / \sqrt{n}}
\end{equation*}
where $\bar{x}_n^*$ and $s_n^*$ are respectively the sample mean and the sample standard deviation of the bootstrap sample. Then, we calculate
\begin{equation*}
    t_0 = \frac{\bar{x}_n - \mu_0}{s_n / \sqrt{n}}
\end{equation*}
i.e., the t-value over the original dataset, and calculate the p-value using the bootstrap distribution of $t^*$. The p-value of the one-sided test is:
\begin{equation*}
    P(T \geq t_0) = \frac{|\{i = 1, \ldots, r : t_i^* \geq t_0 \}|}{r}
\end{equation*}
and the two-sided test is:
\begin{equation*}
    P(|T| \geq |t_0|) = \frac{|\{i = 1, \ldots, r : |t_i^*| \geq |t_0| \}|}{r}
\end{equation*}
where $r$ is the number of repetitions of the bootstrap sampling, which is also the number of estimated $t$ values.

\subsubsection{Bernoulli data: binomial test}

Let $x_1, \ldots, x_n$ be the dataset, realization of $X_1, \ldots, X_n \sim Ber(\theta)$. Let the null and alternative hypotheses be:
\begin{align*}
    &H_0 : \theta = \theta_0 &H_1 : \theta \not = \theta_0
\end{align*}
The test statistic is:
\begin{equation*}
    B = \sum_{i=1}^n X_i \sim Bin(n, \theta_0)
\end{equation*}
This test statistic measures the number of successes in the data; the $b$-value is the actual numer of recorded successes in the dataset, $b = \sum_{i=1}^n x_i$. We can either use the exact distribution and find the critical values, or exploit the normal approximation of the binomial distribution for large $n$.\\
The critical values $l$ and $u$ in the exact test are:
\begin{equation*}
    P(B \leq l) = \sum_{i=0}^l \binom{n}{i} \theta^i_0 (1-\theta_0)^{n-i} = P(B \geq u) = \sum_{i=u}^n \binom{n}{i} \theta^i_0 (1-\theta_0)^{n-i} = \frac{\alpha}{2}
\end{equation*}
If instead we use the normal approximation ($Bin(n, \theta_0) \approx \mathcal{N}(n\theta_0, n\theta_0(1-\theta_0))$), we can build the scaled test statistic:
\begin{equation*}
    B^* = \frac{B - n\theta_0}{\sqrt{n\theta_0(1-\theta_0)}} \sim \mathcal{N}(0,1)
\end{equation*}
and then use the z-test with $\sigma^2 = \theta_0(1-\theta_0)$, since we can rewite the test statistic as:
\begin{equation*}
    B^* = \frac{B/n - \theta_0}{\sqrt{\theta_0(1-\theta_0)/n}} = \frac{\bar{X}_n - \theta_0}{\sigma / \sqrt{n}}
\end{equation*}
Alternatively, we can also use the t-test, if $n$ is sufficiently large.

\subsection{Testing for linear regression}

Let us consider a simple linear regression model:
\begin{align*}
    &Y_i = \alpha + \beta x_i + U_i &U_i \sim \mathcal{N}(0, \sigma^2)
\end{align*}
We set up a two-tailed statistical test for $\beta$. The hypotheses are:
\begin{align*}
    &H_0 : \beta = 0 &H_1 : \beta \not = 0
\end{align*}
We know that $\hat{\beta} \sim \mathcal{N}(\beta, Var{\hat{\beta}})$, where the variance is unknown. The test statistic is
\begin{equation*}
    T = \frac{\hat{\beta} - \beta}{\sqrt{Var(\hat{\beta})}} \sim t(n-2)
\end{equation*}
The p-value is $p = P(|T| > |t|) = 2 \cdot P\left(T > \left|\frac{\hat{\beta} - 0}{se(\hat{\beta})}\right|\right)$. $H_0$ can be rejected at significance level $\alpha$ if $p < \alpha$, or if $|t| > t_{\alpha, n-2}$. \\
A similar approach is used for the intercept.

\subsection{Two sample tests for the mean}
Let $x_1, \ldots, x_n$ be the realizations of $X_1, \ldots, X_n \sim F_1$, with $\E[X_i] = \mu_1$ and $Var(X_i) = \sigma_X^2$. \\
Let $y_1, \ldots, y_m$ be the realizations of $Y_1, \ldots, Y_m \sim F_2$, with $\E[Y_i] = \mu_2$ and $Var(Y_i) = \sigma_Y^2$. \\
How consistent is the dataset with the null hypothesis that the means of the two distributions are equal ($H_0: \mu_1 = \mu_2$)? Like in the one sample case, we can distinguish multiple cases: normal data, general data, bernoulli data, and paired data.

\subsubsection{Normal data}
Let $X_1, \ldots, X_n \sim \mathcal{N}(\mu_1, \sigma_X^2)$ and $Y_1, \ldots, Y_m \sim \mathcal{N}(\mu_2, \sigma_Y^2)$. Assume the test is set up with the following hypotheses:
\begin{align*}
    &H_0 : \mu_1 = \mu_2 &H_1 : \mu_1 \not = \mu_2
\end{align*}
The significance level is $\alpha$. The test statistic is difference depending on what we know about the variances.

\paragraph{Known variance: z-test}
Since $\sigma_X^2$ and $\sigma_Y^2$ are known, we can use the z-test, with the test statistic:
\begin{equation*}
    Z = \frac{\bar{X}_n - \bar{Y}_m}{\sqrt{\frac{\sigma_X^2}{n} + \frac{\sigma_Y^2}{m}}} \sim \mathcal{N}(0,1)
\end{equation*}
The z-value is calculated from the dataset, and the p-value is calculated as $p = P(|Z| \geq |z|) = 2(1-\phi(|z|))$. If the p value is less than $\alpha$, or if the absolute value of the z-value is greater than the critical value $z_{\alpha/2}$ of the standard normal distribution, we reject the null hypothesis.

\paragraph{Unknown variance}
Before proceeding with the test, we need to estimate the variance $Var(\bar{X}_n - \bar{Y}_m) = \sigma^2\left(\frac{1}{n} + \frac{1}{m}\right)$. Variances of the two distributions can be individually (unbiasedly) estimated with the empirical variances as $S_X^2$ and $S_Y^2$. Another important information to gather is whether the variances of the two distributions are equal. For this task, we can use the \textbf{F-test}. The hypotheses are:
\begin{align*}
    &H_0 : \sigma_X^2 = \sigma_Y^2 &H_1 : \sigma_X^2 \not = \sigma_Y^2
\end{align*}
The test statistic is:
\begin{equation*}
    F = \frac{S_X^2}{S_Y^2} \sim F(n-1, m-1)
\end{equation*}
i.e., the ratio of the sample variances. The distribution of $F$ is called \textbf{Fisher-Snedecor distribution}. the f-value is the ratio of the sample variances calculated from the data, and the p-value is $p = 2 \min \{ P(F \leq f), 1 - P(F \leq f) \}$ (the distribution is asymmetric, so the smaller value between the tail probabilities is taken).
Then:
\begin{itemize}
    \item \textbf{If the two variances are equal}, we use the t-test. The test statistic is:
    \begin{equation*}
        T_p = \frac{\bar{X}_n - \bar{Y}_m}{S_p} \sim t(n+m-2)
    \end{equation*}
    where $S_p$ is the pooled satandard deviation. The \textbf{pooled variance} is an unbiased estimator of $Var(\bar{X}_n - \bar{Y}_m)$:
    \begin{equation*}
        S_p^2 = \frac{(n-1) S_X^2 + (m-1) S_Y^2}{n+m-2} \left(\frac{1}{n} + \frac{1}{m}\right) = \frac{\sum_{i=1}^n (X_i - \bar{X}_n)^2 + \sum_{i=1}^m (Y_i - \bar{Y}_m)^2}{n+m-2} \left(\frac{1}{n} + \frac{1}{m}\right)
    \end{equation*}

    \item \textbf{If the two variances are not equal}, we use the Welch t-test. The test statistic is:
    \begin{equation*}
        T_d = \frac{\bar{X}_n - \bar{Y}_m}{S_d} \approx t(v)
    \end{equation*}
    where $S_d$ is the unpooled standard deviation. The \textbf{unpooled variance} is an unbiased estimator of $Var(\bar{X}_n - \bar{Y}^2)$:
    \begin{equation*}
        S_d^2 = \frac{S_X^2}{x} + \frac{S_Y^2}{y}
    \end{equation*}
    The degrees of fredom $v$ are 
    \begin{equation*}
        v = \frac{\left(\frac{1}{n} + \frac{u}{m}\right)^2}{\frac{1}{n^2(n-1)} + \frac{u^2}{m^2(m-1)}} \quad \text{where } u = \frac{S_Y^2}{S_X^2}
    \end{equation*}
\end{itemize}

\subsubsection{General data}
Let $X_1, \ldots, X_n \sim F_1$ and $Y_1, \ldots, Y_m \sim F_2$. The hypotheses are:
\begin{align*}
    &H_0 : \mu_1 = \mu_2 &H_1 : \mu_1 \not = \mu_2
\end{align*}

\paragraph{Large sample size: t-test}
If the sample sizes $n$ and $m$ are large enough, then, by a variant of the CLT, the following test statistic is approximately standard normal:
\begin{equation*}
    T_d = \frac{\bar{X}_n - \bar{Y}_m}{S_d} \sim 
\end{equation*}
where $S_d$ is the unpooled standard deviation.

\paragraph{Location shift: Wilcoxon rank-sum test}
Also known as the Mann-Whitney U test, or Mann-Whitney-Wilcoxon test. This test is used when we want to check if the two distributions are related by a location shift. This means that the hypotheses are:
\begin{align*}
    &H_0 : F_1(x - \delta) = F_2(x) &H_1 : F_1(x - \delta) \not = F_2(x)
\end{align*}
with $\delta = \mu_2 - \mu_1$. The test statistic is:
\begin{equation*}
    W = \sum_{i=1}^n S_i \sim W(n,m)
\end{equation*}
where $S_i$ is the rank of variable $X_i$ in the sorting of the combination of the two datasets. Critical values are extracted from the distribution $W$ (in \texttt{R}, the function \texttt{pwilcox()} is used to calculate its distribution).

\paragraph{Bootstrap t-test}
We distinguish two cases. If the two samples have the same variance, then we use the bootstrap of pooled studentized mean difference:
\begin{equation*}
    t_p^* = \frac{(\bar{x}_n^* - \bar{y}_m^*) - (\bar{x}_n - \bar{y}_m)}{s_p^*} 
\end{equation*}
If instead the variances are different, we use the bootstrap of nonpooled studentized mean difference:
\begin{equation*}
    t_d^* = \frac{(\bar{x}_n^* - \bar{y}_m^*) - (\bar{x}_n - \bar{y}_m)}{s_d^*}
\end{equation*}
Then, we build the test statistic $t_0$ over the original data, and use the empirical distribution of the bootstrap samples to find the critical values.

\subsubsection{Bernoulli data: test of proportions}
Let $X_1, \ldots, X_n \sim Ber(\mu_1)$ and $Y_1, \ldots, Y_m \sim Ber(\mu_2)$. If the sample sizes are large, we define the following test statistic:
\begin{equation*}
    Z = \frac{\bar{X}_n - \bar{Y}_m}{\sqrt{\bar{W}_{n+m} (1 - \bar{W}_{n+m}) \sqrt{\frac{1}{n} + \frac{1}{m}}}} \sim \mathcal{N}(0,1)
\end{equation*}
where $\bar{W}_{n+m} = (X_1 + \ldots + X_n + Y_1 + \ldots + Y_m) / n+m$, i.e., the average of the entire dataset. \\
If the sample sizes are small, we use the \textbf{Fisher's exact test}, based on odds ratios.

\subsubsection{Paired data: t-test}
Let the datasets $x_1, \ldots, x_n$ and $y_1, \ldots, y_n$ be measurements of the same experimental unit; for example, the measurements of blood markers taken from the same person before and after a medical treatment, or a dataset used to train two different classifier models. The idea is to take the differences between each pair of measurements:
\begin{equation*}
    x_1 - y_1, \ldots, x_n - y_n
\end{equation*}
effectively reducing the problem to a one-sample test. The null hypothesis goes from $H_0 : \mu_1 = \mu_2$ to $H_0 : \mu_1 - \mu_2 = 0$. The advantage of this test is that it has better power (lower $\beta$-risk) with respect to the unpaired version.

\subsection{Multiple sample tests for the mean}
Say we want to run multiple tests on the same dataset. For example, we run $m$ tests for the mean of $m$ different subpopulations, and set up a null hypothesis for each of them, claiming that the mean is 0 for each subpopulation:
\begin{equation*}
    H_0^i : \mu_i = 0 \quad i = 1, \ldots, m
\end{equation*}
What is the probability of rejecting at least one $H_0^i$ when all of them are true? If the tests are independent, then this probability is
\begin{equation*}
    P(\cup_{i=1}^m \{p_i \leq \alpha\}) = 1 - P(\cup_{i=1}^m \{p_i > \alpha\}) = 1 - (1 - \alpha)^m
\end{equation*}
If they are dependent, we can state the following:
\begin{equation*}
    P(\cup_{i=1}^m \{p_i \leq \alpha\}) \leq \sum_i P(\{p_i \leq \alpha\}) = m \cdot \alpha
\end{equation*}
\boxdefinition{Family-wise error rate (FWER)}{
    The family-wise error rate is the probability of making at least one Type I error in a family of $m$ tests. If the tests are independent, it is:
    \begin{equation*}
        \alpha_{FWER} = 1 - (1 - \alpha)^m
    \end{equation*}
    If they are dependent:
    \begin{equation*}
        \alpha_{FWER} \leq m \cdot \alpha
    \end{equation*}
}
How can we control the FWER? This is done by choosing the appropriate value of $\alpha$ such that the FWER is guaranteed to be below some bound $b$. There are various methods to do this:
\begin{itemize}
    \item \textbf{Bonferroni correction}: set $\alpha = b/m$, such that $\alpha_{FWER} \leq m \cdot \alpha = b$. Using this method is equivalent to ``scaling down'' the p-values of the tests by $m$, and compare them with $b$ instead of $\alpha$: $p \leq \alpha \iff p \cdot m \leq b$. This method is very conservative, and can lead to a large number of false negatives.
    \item \textbf{Sidak correction}: set $\alpha = 1 - (1 - b)^{\frac{1}{m}}$, such that $\alpha_{FWER} = 1 - (1-\alpha)^m = b$. Like the one above, this method is also equivalent to scaling down the p-values and comparing to $b$. This method is less conservative than the Bonferroni correction, and can be used when the tests are independent.
\end{itemize}
These corrections also help control the false positive rate, since $FWER = P(FP > 0 | H_0^i \ i=1,\ldots,m)$. On the other hand, they increase the false negative rate. We introduce the \textbf{False Discovery Rate}, which is the proportion of false positives among the rejections of the null hypotheses:
\begin{equation*}
    FDR = \frac{FP}{FP + TP}
\end{equation*}
It is associated with a \textbf{q-value}. p-value and q-value are respectively defined as
\begin{align*}
    &p = P(T \geq t | H_0) &q = P(H_0 | T \geq t) 
\end{align*}
The FDR can be controlled by requiring that the q-value is below a certain threshold $b$.

\subsubsection{Omnibus test}
Let the test be set up with the following hypotheses:
\begin{align*}
    &H_0 : \theta_1 = \theta_2 = \ldots = \theta_k = v &H_1 : \theta_i \not = \theta_j \text{for some } i \not = j
\end{align*}
\textbf{Omnibus tests} are used to detect any of several possible differences between the parameters of the distributions. The main advantage of these tests is that they don't require the user to specify which specific pairs are to be compared, and they don't need adjustment for multiple comparisons. Then, if the test is significant ($H_0$ is rejected), we run a \textbf{post-hoc test} to find which specific pairs are different. Post-hoc tests may be \textbf{everything to everything}, which compares all pairs, or \textbf{one to everything}, which compares a new population to the others. We distinguish a few cases with different combinations:
\begin{itemize}
    \item Tests for multiple linear regression, assuming normal errors and equal variances; here we use the F-test and the t-test.
    \item Equality of means with normal distributions and equal variances; here we use ANOVA and Tukey's HSD test/Dunnett's test.
    \item Equality of means with general distributions; here we use the Friedman test and Nemenyi's test.
\end{itemize}

\paragraph{Multiple linear regression: F-test + t-test}
Assume we have a multiple linear regression model in the form:
\begin{equation*}
    \mathbf{Y} = \mathbf{X} \cdot \boldsymbol{\beta} + \mathbf{U}
\end{equation*}
where $\mathbf{Y} = (Y_1, \ldots, Y_n)$, $\mathbf{U} = (U_1, \ldots, U_n)$, and $\mathbf{X} = (\mathbf{x}_1, \ldots, \mathbf{x}_n)$. $\boldsymbol{\beta}$ holds all the coefficients (inlcuding the intercept) of the model. Each vector $\mathbf{x}_i$ has the first element equal to 1. The unexplained residual error is the SSE. The explained error is SSR = SST - SSE. The coefficient of determination is the ratio between SSR and SST, and quantifies how useful the model is in explaining the variance of the data. \\
We want to test whether the model is statistically significant. We set the test hypotheses:
\begin{align*}
    &H_0 : \beta_1 = \ldots = \beta_k = 0 &H_1 : \beta_i \not = 0 \quad \text{for some } i = 1, \ldots, k
\end{align*}
The null hypothesis is assuming the model is a so-called \textbf{null model} (or ``intercept-only model''), where the only predictor is the intercept. The test statistic is:
\begin{equation*}
    F = \frac{SSR}{SSE} \frac{n - k -1}{k} \sim F(k, n-k-1)
\end{equation*}
After the F-test, if the null hypothesis is rejected, we run a t-test to find the specific coefficients that are different from 0.

\paragraph{Equality of means: ANOVA + Tukey/Dunnett}
This test is a generalization of the two sample t-test. The test is set up with the following hypotheses:
\begin{align*}
    &H_0 : \mu_1 = \mu_2 = \ldots = \mu_k &H_1 : \mu_i \not = \mu_j \quad \text{for some } i \not = j
\end{align*}
The data is represented by a series of datasets, $y_1^j, \ldots, y_{n_j}^j$ for $j = 1, \ldots, k$. We assume that the data is normally distributed (can be tested with the Shapiro-Wilk test), and that the variances are equal (can be tested with the Bartlett test). Examples of datasets can be the responses of $k-1$ treatments and 1 control group, or the accuracies of $k$ classifiers over $n_j$ datasets.

We build the linear regression model over the dummy encoded data of the $j^{\textit{th}}$ dataset is:
\begin{equation*}
    Y = \alpha + \beta_1 x_1 + \ldots + \beta_{k-1} x_{k-1}
\end{equation*}
where $\alpha = \mu_k$ is the mean of the reference group, $\beta_j$ is the difference between the mean of the $j^{\textit{th}}$ group and the mean of the reference group, $\mu_j - \mu_k$. We finally run and F-test over this linear regression model, choosing as hypotheses
\begin{align*}
    &H_0 : \beta_1 = \ldots = \beta_k = 0 &H_1 : \beta_i \not = 0 \quad \text{for some } i = 1, \ldots, k
\end{align*}
When all the $\beta_j$ are equal to 0, it means that the difference with the mean of the reference group and the other groups is 0. After this test, we can use either of these tests:
\begin{itemize}
    \item Tukey's Honest Significant Differences, which compares all pairs of means.
    \item Dunnett's test, which is a one-to-everything test.
\end{itemize}

\paragraph{Equality of means: Friedman + Nemenyi}
The test is the same as the on above:
\begin{align*}
    &H_0 : \mu_1 = \mu_2 = \ldots = \mu_k &H_1 : \mu_i \not = \mu_j \quad \text{for some } i \not = j
\end{align*}
The datasets are $x_1^j, \ldots, x_n^j$ for $j=1, \ldots, k$, which are paired observations/repeated measures of the same experimental unit (for example, the accuracies of $k$ classifiers over $n$  datasets). Let $r_i^j$ be the rank of the data point $x_i^j$ within its dataset. We can average the ranks in the whole dataset:
\begin{equation*}
    R_j = \frac{1}{n} \sum_{i=1}^n r_i^j
\end{equation*}
Under the null hypothesis, the ranks over all datasets are equal: $R_1 = R_2 = \ldots = R_k$ (in the classifiers example, this means that they perform equally well). For large $n$ and $k$, we can build the following test statistic:
\begin{equation*}
    \chi_F^2 = \frac{12 n}{k(k+1)} \left( \sum_{j=1}^k  R_j^2 - \frac{k(k+1)^2}{4} \right) \sim \chi^2(k)
\end{equation*}
where $\chi^2(k)$ is the Chi-square distribution with $k$ degrees of freedom.
\distribution{Chi-square distribution}{$X \sim \chi^2(k)$}{
    A random variable has a chi-square distribution with $k$ degrees of freedom is its PDF is given by
    \begin{align*}
        &f(x) = \frac{1}{2^{k/2} \Gamma(\frac{k}{2})} x^{\frac{k}{2} - 1} e^{-\frac{x}{2}} &\text{for } x > 0
    \end{align*}
    The sum of the squares of $k$ i.i.d. standard normal random variables is gamma distributed with parameter $k$:
    \begin{align*}
        &X = \sum_{i=1}^k X_i^2 \sim \chi^2(4) &\text{where } X_i \sim \mathcal{N}(0,1)
    \end{align*}
}
If the null hypothesis is rejected, the Nemenyi test is used to find which specific pairs are different. If unpaired observations are used, the Kruskal-Wallis test is used instead of the Friedman test.

\subsection{s-values}
\textbf{Shannon information values} or (\textbf{s-values}, \textbf{surprisal values}) are a measure of the information of the data against the null hypothesis, and are calculated as $S = -\log_2(p)$, where $p$ is the p-value. The unit of measure of the s-value is the bit.

When comparing p-values of two different test hypotheses, the absolute difference between p-values should not be interpreted ``linearly''. For example, suppose we have two pairs of null hypotheses and respective p-values. The first pair has p-values $0.9$ and $0.9999$, so the difference is $0.0999$; the second pair has p-values $0.0001$ and $0.1$, so the difference is also $0.0999$. However, the first pair correspond to t-values that are really close in the distribution of the test statistic, while the difference between the second pair is more significant. If we take the difference in s-values, we have $< 0.15$ bits and $9.97$ bits of difference respectively. We can interpret this difference as the number of bits of information needed to distinguish between the two hypotheses.