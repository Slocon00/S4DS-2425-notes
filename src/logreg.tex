\section{Logistic regression}

Consider a bivariate dataset $(x_1, y_1), \ldots, (x_n, y_n)$, where $y_i \in \{0,1\}$, i.e., $Y_i$ is a binary variable. The goal is to train a model that predicts to which class (negative = 0, positive = 1) an observation belongs to. If linear regression is used, the model performances will be poor, showing a low $R^2$. This is because the dependent variable is binary, so the trend is not linear. Instead of directly modeling the dependent variable using linear regression, we can instead use it to model the probability of the dependent variable being equal to 1. We can ``transform'' the original dataset into a new one where each distinct value of $x$ is paired with the fraction of 1s found for that value:
\begin{align*}
    &D = (d_1, f_1), (d_2, f_2), \ldots, (d_m, f_m) &\text{where } f_i = \frac{|\{j \in [1,n] : x_j = d_i \land y_j = 1\}|}{|\{j \in [1,n] : x_j = d_i\}|}
\end{align*}
so the linear model can be applied to the new dataset $D$:
\begin{equation*}
    F_i = \alpha + \beta d_i + U_i
\end{equation*}
where $F_i = P(Y_i = 1)$.

Still, the model is not perfect, since the dependent variable is a probability, i.e., bounded between 0 and 1, while the linear regression model returns values in $\mathbb{R}$. Additionally, the relationship between the values of the independent variable(s) and their probability of belonging to the positive class is \textbf{sigmoidal} (S-shaped) and not linear. Rather than $F_i$, we model the \textbf{log odds} of $F_i$:
\begin{equation*}
    \textit{logit}(F_i) = \alpha + \beta d_i + U_i
\end{equation*}
The \textbf{logit} and its inverse, the \textbf{logistic function}, are sigmoidal functions defined as:
\begin{align*}
    &\textit{logit}(p) = \log\left(\frac{p}{1-p}\right) &\textit{inv.logit}(x) = \frac{e^x}{1+e^x} = \frac{1}{1+e^{-x}}
\end{align*}
Since $F_i = P(Y_i = 1)$, and $Y_i$ can take only values 1 with probability $p_i$ or 0 with probability $1-p_i$, the $Y_i$ are Bernoulli distributed with parameter $p_i$, hence the error term $U_i$ is not necessary. The probability $p_i$ can be then retrieved by inverting the logit function:
\begin{equation*}
    p_i = \textit{inv.logit}(\alpha + \beta x_i) = \frac{1}{1 + e^{-(\alpha + \beta x_i)}}
\end{equation*}
Since the distribution is known, MLE can be used to estimate $\alpha$ and $\beta$, where the log likelihood function is:
\begin{gather*}
    \ell(\alpha, \beta) = \log \left(\prod_{i=1}^n p_i^{y_i} (1-p_i)^{(1-y_i)}  \right) = \sum_{i=1}^n (y_i \log p_i + (1-y_i) \log(1-p_i)) =\\
    \sum_{i=1}^n (y_i \log(\textit{inv.logit}(\alpha + \beta x_i)) + (1-y_i) \log(1-\textit{inv.logit}(\alpha + \beta x_i)))
\end{gather*}
Since $p_i/(1-p_i) = e^{\alpha + \beta x_i}$, $e^{\beta}$ can be interpreted as the expected change in the odds of the data point belonging to the positive class for a unit change in $x_i$. When $x_i = 0$, then $e^{\alpha}/(1 + e^{\alpha})$ can be interpreted as the base probability of the data point belonging to the positive class.

Logistic regression, as well as linear regression, belongs to the family of \textbf{generalized linear models} (\textbf{GLM}), in which the outcome of the dependent variable is assumed to be generated by a particular distribution, and a link function is used to relate the mean of the distribution to the linear model. In the case of binary logistic regression, the distribution is Bernoulli and the link function is the logit function.

Logistic regression can also be regularized by adding penalty terms to the log likelihood. \textbf{Elastic net} regularization is a combination of L1 and L2 regularization, where the function to be minimized is:
\begin{equation*}
    - \ell(\boldsymbol{\beta}) + \lambda \left( \frac{1-\alpha}{2} \|\boldsymbol{\beta}\|^2 + \alpha \|\boldsymbol{\beta}\|_1\right)
\end{equation*}
where:
\begin{itemize}[noitemsep]
    \item $\alpha = 0$ is equivalent to Ridge regression (L2 regularization);
    \item $\alpha = 1$ is equivalent to Lasso regression (L1 regularization);
    \item $0 < \alpha < 1$ is a combination of both.
\end{itemize}